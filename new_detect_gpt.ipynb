{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## imports\n",
        "import os"
      ],
      "metadata": {
        "id": "jR32WRviVjcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dX5dVDSRCkA",
        "outputId": "8207e620-e678-46ee-97f2-75d2541ebca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PAF3NCrG0b7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !git clone https://github.com/Dada-Tech/detect-gpt.git\n",
        "# !git pull\n",
        "\n",
        "# https://github.com/Dada-Tech/detect-gpt.git\n",
        "# https://github.com/eric-mitchell/detect-gpt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## directories\n",
        "directory = 'detect-gpt-main'\n",
        "base_dir = '/content/drive/MyDrive/CS6220_NLP/'\n",
        "\n",
        "if os.getcwd() != base_dir + directory:\n",
        "  os.chdir(base_dir + 'detect-gpt-main')\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "## drive mounts\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eJR3ywcDVkXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0b18a1-0f4d-41bc-fb17-0c57a9d8362d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS6220_NLP/detect-gpt-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## dir deletion\n",
        "\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/detect-gpt', ignore_errors=True)"
      ],
      "metadata": {
        "id": "S3BtQlZCupxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## requirements\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "wO6pzxhVXGs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2713fd2c-0082-4bcc-bb16-84c2954d7735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (1.22.4)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (16.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 3)) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 3)) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 3)) (23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 4)) (2023.4.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 4)) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 4)) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 5)) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 5)) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 5)) (5.12.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.10.1)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (22.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 5)) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->-r requirements.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Installing collected packages: tokenizers, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, openai, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.4 multidict-6.0.4 multiprocess-0.70.14 openai-0.27.4 responses-0.18.0 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU enabled\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "-jib2hBSp7tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b92d0b8-9d60-4d7d-abb1-9ba787c34d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 16 17:54:24 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## script run (main.sh)\n",
        "\n",
        "!./paper_scripts/main.sh"
      ],
      "metadata": {
        "id": "Yq6I7NV_WuZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85872a4-df9d-4945-bd65-4d4b1dcb68fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: ./paper_scripts/main.sh: Permission denied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## manual run\n",
        "\n",
        "!python run.py --batch_size 5 --n_samples 200 --n_perturbation_list 10 --base_model_name gpt2 --mask_filling_model_name t5-small --dataset books --cache_dir cache_code_books\n",
        "# !python run.py --batch_size 5 --n_samples 100 --n_perturbation_list 1,3 --dataset reviews --cache_dir code\n"
      ],
      "metadata": {
        "id": "Nzoj4_ZNXsrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d14a274-bbb1-425f-9cf1-73ad365f17de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to absolute path: /content/drive/MyDrive/CS6220_NLP/detect-gpt-main/tmp_results/gpt2-t5-small-temp/2023-04-16-17-57-55-180920-fp32-0.3-1-books-200\n",
            "Using cache dir cache_code_books\n",
            "Loading BASE model gpt2...\n",
            "Loading mask filling model t5-small...\n",
            "Downloading pytorch_model.bin: 100% 242M/242M [00:01<00:00, 191MB/s]\n",
            "Downloading (…)neration_config.json: 100% 147/147 [00:00<00:00, 25.7kB/s]\n",
            "MOVING BASE MODEL TO GPU...DONE (0.56s)\n",
            "Loading dataset books...\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
            "Total number of samples: 204\n",
            "Average number of words: 154.3578431372549\n",
            "Generating samples for batch 0 of 40\n",
            "2023-04-16 17:58:20.645792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Generating samples for batch 1 of 40\n",
            "Generating samples for batch 2 of 40\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 3 of 40\n",
            "Generating samples for batch 4 of 40\n",
            "Generating samples for batch 5 of 40\n",
            "Generating samples for batch 6 of 40\n",
            "Generating samples for batch 7 of 40\n",
            "Generating samples for batch 8 of 40\n",
            "Generating samples for batch 9 of 40\n",
            "Generating samples for batch 10 of 40\n",
            "Generating samples for batch 11 of 40\n",
            "Generating samples for batch 12 of 40\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 13 of 40\n",
            "Generating samples for batch 14 of 40\n",
            "Generating samples for batch 15 of 40\n",
            "Generating samples for batch 16 of 40\n",
            "Generating samples for batch 17 of 40\n",
            "Generating samples for batch 18 of 40\n",
            "Generating samples for batch 19 of 40\n",
            "Generating samples for batch 20 of 40\n",
            "Generating samples for batch 21 of 40\n",
            "Generating samples for batch 22 of 40\n",
            "Generating samples for batch 23 of 40\n",
            "Generating samples for batch 24 of 40\n",
            "Generating samples for batch 25 of 40\n",
            "Generating samples for batch 26 of 40\n",
            "Generating samples for batch 27 of 40\n",
            "Generating samples for batch 28 of 40\n",
            "Generating samples for batch 29 of 40\n",
            "Generating samples for batch 30 of 40\n",
            "Generating samples for batch 31 of 40\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 32 of 40\n",
            "Generating samples for batch 33 of 40\n",
            "Generating samples for batch 34 of 40\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 35 of 40\n",
            "Generating samples for batch 36 of 40\n",
            "Generating samples for batch 37 of 40\n",
            "Generating samples for batch 38 of 40\n",
            "Generating samples for batch 39 of 40\n",
            "Writing raw data to tmp_results/gpt2-t5-small-temp/2023-04-16-17-57-55-180920-fp32-0.3-1-books-200/raw_data.json\n",
            "Computing likelihood criterion: 100% 40/40 [00:07<00:00,  5.28it/s]\n",
            "likelihood_threshold ROC AUC: 0.9168625000000001, PR AUC: 0.912692705251232\n",
            "Computing rank criterion: 100% 40/40 [00:11<00:00,  3.59it/s]\n",
            "rank_threshold ROC AUC: 0.8561125, PR AUC: 0.8855818236425463\n",
            "Computing log_rank criterion: 100% 40/40 [00:10<00:00,  3.66it/s]\n",
            "log_rank_threshold ROC AUC: 0.9463375, PR AUC: 0.9542293744668124\n",
            "Computing entropy criterion: 100% 40/40 [00:08<00:00,  4.88it/s]\n",
            "entropy_threshold ROC AUC: 0.43018750000000006, PR AUC: 0.4558400030236196\n",
            "Beginning supervised evaluation with roberta-base-openai-detector...\n",
            "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Evaluating real: 100% 40/40 [00:02<00:00, 13.84it/s]\n",
            "Evaluating fake: 100% 40/40 [00:02<00:00, 15.57it/s]\n",
            "roberta-base-openai-detector ROC AUC: 0.9838625000000001, PR AUC: 0.9842372303563952\n",
            "Beginning supervised evaluation with roberta-large-openai-detector...\n",
            "Some weights of the model checkpoint at roberta-large-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Evaluating real: 100% 40/40 [00:09<00:00,  4.18it/s]\n",
            "Evaluating fake: 100% 40/40 [00:09<00:00,  4.31it/s]\n",
            "roberta-large-openai-detector ROC AUC: 0.99515, PR AUC: 0.995258101178845\n",
            "MOVING MASK MODEL TO GPU...DONE (0.20s)\n",
            "Applying perturbations:  23% 23/100 [00:17<00:55,  1.38it/s]WARNING: 1 texts have no fills. Trying again [attempt 1].\n",
            "Applying perturbations: 100% 100/100 [01:14<00:00,  1.35it/s]\n",
            "Applying perturbations: 100% 100/100 [01:14<00:00,  1.35it/s]\n",
            "MOVING BASE MODEL TO GPU...DONE (0.20s)\n",
            "Computing log likelihoods: 100% 200/200 [01:26<00:00,  2.30it/s]\n",
            "perturbation_10_d ROC AUC: 0.7027000000000001, PR AUC: 0.6760951793865982\n",
            "WARNING: std of perturbed original is 0, setting to 1\n",
            "Number of unique perturbed original texts: 1\n",
            "Original text: Collects The Punisher #1-6.\n",
            "WARNING: std of perturbed sampled is 0, setting to 1\n",
            "Number of unique perturbed sampled texts: 1\n",
            "Sampled text: Collects The Punisher #1-6.\n",
            "\n",
            "-\n",
            "WARNING: std of perturbed original is 0, setting to 1\n",
            "Number of unique perturbed original texts: 1\n",
            "Original text: COLLECTING: MARVEL UNIVERSE VS. THE PUNISHER #1-4\n",
            "WARNING: std of perturbed sampled is 0, setting to 1\n",
            "Number of unique perturbed sampled texts: 1\n",
            "Sampled text: COLLECTING: MARVEL UNIVERSE VS. THE PUNISHER #1-4\n",
            "\n",
            "Written\n",
            "WARNING: std of perturbed original is 0, setting to 1\n",
            "Number of unique perturbed original texts: 1\n",
            "Original text: Collects Captain Marvel #1-6.\n",
            "WARNING: std of perturbed sampled is 0, setting to 1\n",
            "Number of unique perturbed sampled texts: 1\n",
            "Sampled text: Collects Captain Marvel #1-6.\n",
            "\n",
            "Share\n",
            "WARNING: std of perturbed original is 0, setting to 1\n",
            "Number of unique perturbed original texts: 1\n",
            "Original text: Collects Web of Spider-Man #31-32, Amazing Spider-Man #293-294, and Spectacular Spider-Man #131-132.\n",
            "WARNING: std of perturbed sampled is 0, setting to 1\n",
            "Number of unique perturbed sampled texts: 1\n",
            "Sampled text: Collects Web of Spider-Man #31-32, Amazing Spider-Man #293-294, and Spectacular Spider-Man #131-132.\n",
            "perturbation_10_z ROC AUC: 0.655375, PR AUC: 0.6331976971780133\n",
            "perturbation_10_d roc_auc: 0.703\n",
            "perturbation_10_z roc_auc: 0.655\n",
            "likelihood_threshold roc_auc: 0.917\n",
            "rank_threshold roc_auc: 0.856\n",
            "log_rank_threshold roc_auc: 0.946\n",
            "entropy_threshold roc_auc: 0.430\n",
            "roberta-base-openai-detector roc_auc: 0.984\n",
            "roberta-large-openai-detector roc_auc: 0.995\n",
            "Used an *estimated* 0 API tokens (may be inaccurate)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run.py --output_name n_perturb --base_model_name gpt2 --mask_filling_model_name t5-small --n_perturbation_list 10 --n_samples 200 --pct_words_masked 0.3 --span_length 2 --dataset books --cache_dir cache_code_books"
      ],
      "metadata": {
        "id": "yTP2gWirXDsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43009d1a-eae3-47c2-ede1-18de73884e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to absolute path: /content/drive/MyDrive/CS6220_NLP/detect-gpt-main/tmp_results/n_perturb/gpt2-t5-small-temp/2023-04-16-18-24-34-023677-fp32-0.3-1-books-200\n",
            "Using cache dir cache_code_books\n",
            "Loading BASE model gpt2...\n",
            "Loading mask filling model t5-small...\n",
            "MOVING BASE MODEL TO GPU...DONE (0.38s)\n",
            "Loading dataset books...\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
            "Total number of samples: 204\n",
            "Average number of words: 154.3578431372549\n",
            "Generating samples for batch 0 of 4\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "2023-04-16 18:24:48.487719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Generating samples for batch 1 of 4\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating samples for batch 2 of 4\n",
            "Generating samples for batch 3 of 4\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Writing raw data to tmp_results/n_perturb/gpt2-t5-small-temp/2023-04-16-18-24-34-023677-fp32-0.3-1-books-200/raw_data.json\n",
            "Computing likelihood criterion: 100% 4/4 [00:07<00:00,  1.90s/it]\n",
            "likelihood_threshold ROC AUC: 0.903475, PR AUC: 0.9027366852869216\n",
            "Computing rank criterion: 100% 4/4 [00:10<00:00,  2.64s/it]\n",
            "rank_threshold ROC AUC: 0.8521749999999999, PR AUC: 0.8807319975552448\n",
            "Computing log_rank criterion: 100% 4/4 [00:10<00:00,  2.75s/it]\n",
            "log_rank_threshold ROC AUC: 0.937425, PR AUC: 0.9461852231525889\n",
            "Computing entropy criterion: 100% 4/4 [00:07<00:00,  1.99s/it]\n",
            "entropy_threshold ROC AUC: 0.446675, PR AUC: 0.4779521433009352\n",
            "Beginning supervised evaluation with roberta-base-openai-detector...\n",
            "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Evaluating real: 100% 4/4 [00:02<00:00,  1.47it/s]\n",
            "Evaluating fake: 100% 4/4 [00:02<00:00,  1.90it/s]\n",
            "roberta-base-openai-detector ROC AUC: 0.9811, PR AUC: 0.981279767494895\n",
            "Beginning supervised evaluation with roberta-large-openai-detector...\n",
            "Some weights of the model checkpoint at roberta-large-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Evaluating real: 100% 4/4 [00:08<00:00,  2.24s/it]\n",
            "Evaluating fake: 100% 4/4 [00:06<00:00,  1.74s/it]\n",
            "roberta-large-openai-detector ROC AUC: 0.99555, PR AUC: 0.9954984278501376\n",
            "MOVING MASK MODEL TO GPU...DONE (0.20s)\n",
            "Applying perturbations:  99% 99/100 [01:12<00:00,  1.11it/s]WARNING: 1 texts have no fills. Trying again [attempt 1].\n",
            "Applying perturbations: 100% 100/100 [01:13<00:00,  1.36it/s]\n",
            "Applying perturbations: 100% 100/100 [01:12<00:00,  1.39it/s]\n",
            "MOVING BASE MODEL TO GPU...DONE (0.24s)\n",
            "Computing log likelihoods: 100% 200/200 [01:27<00:00,  2.28it/s]\n",
            "perturbation_10_d ROC AUC: 0.7408, PR AUC: 0.7270861259678886\n",
            "WARNING: std of perturbed original is 0, setting to 1\n",
            "Number of unique perturbed original texts: 1\n",
            "Original text: Collects The Punisher #1-6.\n",
            "WARNING: std of perturbed sampled is 0, setting to 1\n",
            "Number of unique perturbed sampled texts: 1\n",
            "Sampled text: Collects The Punisher #1-6.\n",
            "\n",
            "\"Mystic\n",
            "WARNING: std of perturbed original is 0, setting to 1\n",
            "Number of unique perturbed original texts: 1\n",
            "Original text: COLLECTING: MARVEL UNIVERSE VS. THE PUNISHER #1-4\n",
            "WARNING: std of perturbed sampled is 0, setting to 1\n",
            "Number of unique perturbed sampled texts: 1\n",
            "Sampled text: COLLECTING: MARVEL UNIVERSE VS. THE PUNISHER #1-4\n",
            "\n",
            "Written\n",
            "WARNING: std of perturbed original is 0, setting to 1\n",
            "Number of unique perturbed original texts: 1\n",
            "Original text: Collects Captain Marvel #1-6.\n",
            "WARNING: std of perturbed sampled is 0, setting to 1\n",
            "Number of unique perturbed sampled texts: 1\n",
            "Sampled text: Collects Captain Marvel #1-6.\n",
            "\n",
            "-\n",
            "WARNING: std of perturbed original is 0, setting to 1\n",
            "Number of unique perturbed original texts: 1\n",
            "Original text: Collects Web of Spider-Man #31-32, Amazing Spider-Man #293-294, and Spectacular Spider-Man #131-132.\n",
            "WARNING: std of perturbed sampled is 0, setting to 1\n",
            "Number of unique perturbed sampled texts: 1\n",
            "Sampled text: Collects Web of Spider-Man #31-32, Amazing Spider-Man #293-294, and Spectacular Spider-Man #131-132.\n",
            "\n",
            "Marvel\n",
            "perturbation_10_z ROC AUC: 0.6727500000000001, PR AUC: 0.6396552538727178\n",
            "perturbation_10_d roc_auc: 0.741\n",
            "perturbation_10_z roc_auc: 0.673\n",
            "likelihood_threshold roc_auc: 0.903\n",
            "rank_threshold roc_auc: 0.852\n",
            "log_rank_threshold roc_auc: 0.937\n",
            "entropy_threshold roc_auc: 0.447\n",
            "roberta-base-openai-detector roc_auc: 0.981\n",
            "roberta-large-openai-detector roc_auc: 0.996\n",
            "Used an *estimated* 0 API tokens (may be inaccurate)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouUQhqZHTwxR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}